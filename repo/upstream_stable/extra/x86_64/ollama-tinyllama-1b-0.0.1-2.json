{"filename": "ollama-tinyllama-1b-0.0.1-2-any.pkg.tar.zst", "name": "ollama-tinyllama-1b", "base": "ollama-tinyllama-1b", "version": "0.0.1-2", "desc": "The tinyllama (1B) large language model (LLM), for Ollama", "csize": "605302787", "isize": "637700990", "sha256sum": "42c0e688177b980786de319f776dd932f9215653cc955563b3d55d12a1345dcc", "pgpsig": "iQIzBAABCgAdFiEEipvFgZxU/rPcKptIwyIX9vE/8ZIFAmcAIVIACgkQwyIX9vE/8ZKKtA/7BO42UcDa5pP4Oryz80BW3Ks2zHgGikE4SF3NlZSgZwMnpTMxI+5Zcmivyjh0IUvfcd94XPIooNo/1rmTglBBuW5RvMVR6GQbH/rmv4+ZGkMTJvs6WUv5FrY9Ju8Tg3bbho7OhheKazT402rpxDUUD1NX7GOLGlHFRChmLvI3bJ+gk6NjLCTDp9aQXoB2x+qwzj1hKB94fozKTo9mSyvq8Pc9ij4gwC9J+SnF/8vvH+alZauhoqjkEZwTpV/lB20k/ZdouHzI6m/NrlG5Ij6gfWlbUT250l0QQyjuH1R6xPzhN90Y6H07OuSUiQK2Jv+JYH1/Fp+o9YOu6L+P3b8jgbDVzrOoTAjbCSIcnUdVlxyFv8UoTEq3BBvjB+RUlg/lZ/B6EQps+mrG7Q++afhjBgINTLSwngF055FZEuWXUXO5sqInLRx7r/EbSxefQ0INPRHBh5DXMdaBEiuhCyPNJORXAHHrDvDIcI9wktzznX/BfoKNBRDqVyXR8XFkBx2Shb1tCEuV14AU+aoGYfJGAdMLCZcVm34AfNIqkt2EugiKV6PtQf9jADgO5fXitR41JHap4xKik41nwqE5qHn+1EtFCnMMs/0tnBi65nAUnSSpOENmfx36pJAhw4mivBQhaFdTZfyQylyM6WIqZhyVLmtFsioJRE8ElPYgpbwcDrk=", "url": "https://github.com/jzhang38/TinyLlama", "license": "Apache-2.0", "arch": "any", "builddate": "1728061686", "packager": "Alexander F. R\u00f8dseth <xyproto@archlinux.org>", "provides": ["ollama-tinyllama"], "depends": ["ollama"], "optdepends": ["ollama-cuda: for using the GPU", "llm-manager: for configuring which models to use for which tasks", "ollama-rocm: for using the GPU"]}