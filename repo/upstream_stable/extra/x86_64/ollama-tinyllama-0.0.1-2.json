{"filename": "ollama-tinyllama-0.0.1-2-any.pkg.tar.zst", "name": "ollama-tinyllama", "base": "ollama-tinyllama", "version": "0.0.1-2", "desc": "The tinyllama (1B) large language model (LLM), for Ollama", "csize": "605169824", "isize": "637700989", "sha256sum": "9bc6c0318124e7e843bb5cd215d5806d4d11f1dc98efc8be3b57d4f8b489b470", "pgpsig": "iQIzBAABCgAdFiEEipvFgZxU/rPcKptIwyIX9vE/8ZIFAmbYb84ACgkQwyIX9vE/8ZJcQw//XN1UYU0hCvtvpiu3JVSnSDV0EA1ntkxHp6XNmC9GN7VLqnx8oY/slqpbBl426XsfzTDs/ok25gPPabQ7lnKJ08M4XWNhPEjdMcFjeTDURbekiAEuK95JKNm3XbHBBfbik+7BHHJJAKyeyiEG38qfeHcvL/37v61YdaEiDq3IWfJQqiowIXSrliYZnMh+ukqmghIshOsvCgx0RXhtfgz58bWj7FOB8++cNrTJmkNtiXxqeRXrnclO9/70f0U8WJIwgWD4K9am3RG1S8njG0rhuHHlCeW3L+eGt9SYT31EUsg6iBWg89Lm4WYI8XYApmLJXTJBMm+x/3txXGqVWeY1IESghw8VIpt4+IRS8yU79qQPd27LEIpSq5N0xnY9J8TS7S2dEs6jRk/dAlltQYqbEwGIbljMqBVw4J7sOHVtUMg0hnmfkJDeoAW2mVsEKfpwjIvr7hT7u8pkjM/aIIfphFDgmnnXiHpw6/iZg6375wqcFFaLIjAgGIji5ClGc+HJlggEWcOxUVEd5UFGW1ZFP7wk3nElHg3A4lxOeDjuNj15MctXW0kGE2hBVhi+1awgdOYsgphKUdleGuqIUtOb/rtkpIdgW4/mANTwZouwyduHboSTXevVEYa5U5UB1dyQEjZZX2SBJ/TNT4w9w9QcvABCki6QZw3xlxEFBfjU2Pc=", "url": "https://github.com/jzhang38/TinyLlama", "license": "Apache-2.0", "arch": "any", "builddate": "1725460278", "packager": "Alexander F. R\u00f8dseth <xyproto@archlinux.org>", "depends": ["ollama"], "optdepends": ["ollama-rocm: for using the GPU", "ollama-cuda: for using the GPU"], "makedepends": ["python"]}