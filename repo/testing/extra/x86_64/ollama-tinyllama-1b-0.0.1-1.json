{"filename": "ollama-tinyllama-1b-0.0.1-1-any.pkg.tar.zst", "name": "ollama-tinyllama-1b", "base": "ollama-tinyllama-1b", "version": "0.0.1-1", "desc": "The tinyllama (1B) large language model (LLM), for Ollama", "csize": "605168446", "isize": "637700989", "md5sum": "8d8fbbc8ad61ee85ee3a022f1850a187", "sha256sum": "1c1b507af257c56ed181644df6439465ea22cbb87d13c54ccf61ba97f319aaf2", "pgpsig": "iQIzBAABCgAdFiEEipvFgZxU/rPcKptIwyIX9vE/8ZIFAmbYdxQACgkQwyIX9vE/8ZL9Pg/9GOwfg+caLqq81puti9VriN4i335h+fvIViVpmg58zoH9GSvUcHti1+i9fx3qW/i2c54dmawuOiltbjoGXVtYmZVMR3AmjSefTaiBZpZJmYqrbXfJG88xpEanyMcWSjY7dxps9lgjkUCKRPZBiYthc5DDqc28pLJenYdvjIFdFMRc9YwCaAxJ7HB0fdT2QxEsTx0C91WprTN2lV1V+XDkRK9sZqH3+GWsYHjN9lE9bYxBJKefCfUVWzfI3Ib66DoKw+2N6CFMBK+IO1Y2A2C24cceVKfa8rJWYnAnJ0csKauWQ2FG0h4QnuBoLjQ1JEE9pPabJjm16xQHFLxtU/owmZawXplEpG6NaqOxP0STm+aOgwOaEttdogjFEgW4OwRnYZ39DrsIoS09i8Hk5L+g5kgFc+YPOcqUms3ozz6lsVQWYmawdmW1Hs38sy+l4Arrv+caJA+gYs7m7NxnT1KIke44yk4pc8fzeA/2WfQ3ALqrnPgnlfXb4hyipHmy0LbIAievkeh9H/1pimFruxSLmvPedryhLGLV/2cpHYDVYexyM94cM8fnnIKDC7s8yfixXPxM+3Xkbic1tdHI3zlhWS2KXJD2sfEnrBW0wvPoW72J10v5s4sSW2Z4Bu1EUE1Rqt9HPBCP5g5tVKiSAodKS/gQ/8lASwQukVKCVniAqk4=", "url": "https://github.com/jzhang38/TinyLlama", "license": "Apache-2.0", "arch": "any", "builddate": "1725461480", "packager": "Alexander F. R\u00f8dseth <xyproto@archlinux.org>", "provides": ["ollama-tinyllama"], "depends": ["ollama"], "optdepends": ["ollama-rocm: for using the GPU", "ollama-cuda: for using the GPU"], "makedepends": ["python"]}