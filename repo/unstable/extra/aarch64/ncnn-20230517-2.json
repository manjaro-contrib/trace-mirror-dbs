{"filename": "ncnn-20230517-2-aarch64.pkg.tar.xz", "name": "ncnn", "base": "ncnn", "version": "20230517-2", "desc": "High-performance neural network inference framework optimized for the mobile platform", "csize": "2534080", "isize": "11547882", "md5sum": "4e8a93c6bc65e5d087fd2caf3881d501", "sha256sum": "e0f7aa82dceb4a21a7199a3ce674d2abb512edd487db7fe56ce494d63cba0090", "pgpsig": "iQIzBAABCAAdFiEEaLNTfzmjE7PldNBndxk/FSvb5qYFAmSkR3IACgkQdxk/FSvb5qZZIRAAn/PLBLflxb3rNxDy34DvnK1+BBDlCgAF14K9HSdt8E5GqqseDiEHjEYBrCAsSRkS2PQ94G4owWtIKB/jlMu//c1mWBl0212bTVlMiqTW04w82IXhxIxJgZPuqlAke+XfYglzKaZtAffdC0rcNkTyUeeThcsrqo0228sjl2lXg8ptP1mzWmK28mQHt2+vY+LskyjM3H5grM3pZNbaLTJTSMyIAObvuVi128Ro8AQBelTphWS8DtCxcGtcONv/YDdMwIN4TpRJNvk5hpGAXl3dUdIxsUEOirqo1TxLsssd7VQbWL0PCsLcpFjjFJa8Po3th+rfZeEkwOl+ghoxIYjiw5HyuW2ry0mOysKAv0Dxk+mxkBUqzQVi8zD1Fj6/J3/Ub8FoiPpOanLxh4Jl1bc0TkmlN2tXLq3dDtOrmuZJGzY76D+IyxVsBxgQ8RvIpA5WzhVZoSw7Fhluv/kjSCTK2V0JjNdJDW9wW0tW9dfmogSpepo0h9jsfZBTCwetnmfVInEDA8HL630gpOGMShsm+C53i51GZILUfDWbP+VlKMW4ordg4hCqTlEw/JyVK7tVmmJtSAuzn5DGWuu3WfnyAilhRv5jyrfaWuqQt/OEQY242xowCj7Yovyuwu6n2AYC94iFrz4Tr71tSsU+eQCnf9r05uWz2uyxqHAqViWBDog=", "url": "https://github.com/Tencent/ncnn", "license": "BSD", "arch": "aarch64", "builddate": "1688487652", "packager": "Arch Linux ARM Build System <builder+seattle@archlinuxarm.org>", "conflicts": "ncnn-git", "depends": ["vulkan-icd-loader"], "optdepends": "protobuf: for onnx2ncnn", "makedepends": ["cmake", "glslang", "ninja", "protobuf", "vulkan-headers"], "tokens": ["ncnn", "ncnn_aarch64", "ncnn_unstable", "ncnn_aarch64_unstable"]}