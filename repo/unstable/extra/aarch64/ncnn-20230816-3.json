{"filename": "ncnn-20230816-3-aarch64.pkg.tar.xz", "name": "ncnn", "base": "ncnn", "version": "20230816-3", "desc": "High-performance neural network inference framework optimized for the mobile platform", "csize": "2507608", "isize": "11434570", "md5sum": "1e114c329baefc48dfe5cc9580426c78", "sha256sum": "f29a2ae252e541a38b21ace32e58cf9e2e35061d8457f62fb43eb15647813a01", "pgpsig": "iQIzBAABCAAdFiEEaLNTfzmjE7PldNBndxk/FSvb5qYFAmVJmFwACgkQdxk/FSvb5qZXdQ/6Aninp8R5ILsPvR57FfZl57rHhEHu5GFlwhsFTTt/hK0wei05obOI6LW6uPYTLv5Jps6CzRLR6z/ngseKxwNBp3jDzoOdX8HQY9t7bVSS6nEoWasNbKMekrhv/PRyRsC8g+i90jlKbLdD8KLnN5XDjRzMNPosnkPaMAdx8Gr8wEY08eLQOCMmS/nMeVHg6g9Bf0dymn28N0dG4EAvDhZnrmNZL9COJdiwIuqOLQ6OXAvFHuYnPDSV2GBgIrW5oOGO7RVL9le9RQewMIJJTSw/QVMhe9dlR/WYFlfJAPGUs6kzh8xWBZghTTpf77gHDROcN8oISmclAMas4W6KJNiq/tH4BEC6Tcx8zKlyf2YLbpb5B6Mli0PnnwFGUT/B3UmcJ4XW+3lJn/t69O50fifzBsf6Tzq/JEGdHevK5NsPPqDEZMJv5faNm9De6AggZw1ZQm9SuhqeuSGfRihjtaoDhbVNFF46Aeonq1acZZ14iTEuPfPM/CQdePPGvG0L3negVCafjtKuvNSD/U9ROmG7WKwjjsC4eHVcIJ4UXEHY89iS37cpGAPci/sl2+m5FOgpAAPfKTzPeTli/1yoMF+4bjTM0tpyDhHeZ+WEWTXNfeiQqAhGT2QjGQdzUXnWqlDizjBLppgOjGAoDvBqeoy0eULuVKew6uXYPW4sWPzv5Dw=", "url": "https://github.com/Tencent/ncnn", "license": "BSD", "arch": "aarch64", "builddate": "1699321812", "packager": "Arch Linux ARM Build System <builder+seattle@archlinuxarm.org>", "conflicts": "ncnn-git", "depends": ["vulkan-icd-loader"], "optdepends": "protobuf: for onnx2ncnn", "makedepends": ["cmake", "glslang", "ninja", "protobuf", "vulkan-headers"], "tokens": ["ncnn", "ncnn_aarch64", "ncnn_unstable", "ncnn_aarch64_unstable"]}